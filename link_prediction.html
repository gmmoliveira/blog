<!DOCTYPE html>
<html>
	<head>
		<meta charset="UTF-8">
		<meta name="description" content="scientific/professional blog">
		<meta name="author" content="Guilherme Oliveira">

		<title>Guilherme Oliveira</title>

		<link rel="stylesheet" type="text/css" href="all.css">
		<script src="https://code.jquery.com/jquery-3.5.1.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
		<script type="text/javascript" src="all.js"></script>

		<link rel="stylesheet" href="styles/atelier-lakeside-dark.css">
		<script src="highlight.pack.js"></script>
		<script>hljs.initHighlightingOnLoad();</script>
	</head>
	<body>
		<div id="top_img">

			<div id="top_menu">

				<ul id="ul_top_menu">
					<li class="li_top_menu"><a href="index.html"><b>Home & Appendix</b></a></li>
					<li class="li_top_menu"><a href="cv.html"><b>CV</b></a></li>
					<li class="li_top_menu"><a href="contact.html"><b>Contact</b></a></li>
					<li class="li_top_menu"><a href="license.html"><b>Licenses</b></a></li>
				</ul>

				<div id="lower_top">
					<img id="face" src="resources/face.jpg">
					<div id="name">Guilherme Oliveira</div>
					<h1 id="page_title">Graph learning: link prediction using graph embeddings</h1>
				</div>
			</div>
		</div>
		<div id="main_text">

            <h3>Introduction</h3>
            Learning tasks on structured data such as graphs may be achieved using different approaches, in this
            article, we'll discuss a technique using embeddings for link prediction.
            Link prediction refers to predicting the existence of unknown edges in a given graph, while embeddings are
            multidimensional arrays
            containing real values, they are representations for each vertex in a graph. Further, we will be working with
            the <a href="https://github.com/tpimentelms/nbne" rel="noopener noreferrer" target="_blank">
            NBNE (Neighbor Based Node Embeddings)</a> over an anonymized undirected and unweighted
            <a href="https://snap.stanford.edu/data/egonets-Facebook.html" rel="noopener noreferrer" target="_blank">
            example graph extracted from the facebook</a>,
            containing more than 4 thousand vertices and 88 thousand edges. The
            <a href="https://github.com/gmmoliveira/link-prediction" rel="noopener noreferrer" target="_blank">
            source-code for this example of the link prediction task</a> is also provided at my github.
            <h3>Overview: how the NBNE and the link prediction works</h3>
            The NBNE is a robust and fast algorithm for learning graph embeddings compared to others
            which are based on random walk techniques. Behind the scenes it uses the skipgram algorithm which was
            originally designed to handle text, however, a text might be seen as a graph where each vertex is a word
            and edges connect co-occurring words in a given context. Using this approach to it's advantage, the NBNE
            also creates
            permutations of the neighbors of each vertex, obeying a fixed window size, which is far more efficient
            and effective than the random walk based methods. The main idea on creating the embeddings is to try predicting
            a node given a set of it's neighbors. The embeddings produced may have arbitrary number of dimensions,
            in addition, those embeddings become the new representation for each node.
            <br>
            Since the embeddings are representations of the graph nodes, we may use any classification algorithm to
            predict whether there should be a link between 2 given nodes in the graph. Therefore, after generating the
            embeddings for each node, all we must do is repeatedly randomly select edges or non-edges composed of
            vertices
            <code class="inline_code">u</code> and <code class="inline_code">v</code>, concatenate their embeddings
            forming a single larger array so this becomes a features array and, finally, set it's class to
            correspond to whether it's an edge or non-edge in the input graph.
            Just to clarify, a non-edge is the absence of an edge (or link)
            between two vertices. It's very important to notice a few key points to this approach:
            <ul>
                <li>not all edges and non-edges should be selected to compose the training data, the idea is to train
                    the classifier with limited information on the graph. This strategy enables predicting links
                    over all non presented cases of edges and non-edges;
                </li>
                <li>
                    the links and non-links should be selected at random obeying the same proportion. Consider, again, our
                    facebook example graph with 4039 nodes and 88234 links. It's non-links would be all the links
                    in the complementary graph (without self loops), i. e.,
                    <code class="inline_code">4039 <sup>2</sup> - 4039 - 88234 = 16221248</code> non-links. Therefore,
                    if we define our links selection rate to be 10%, then we must select for training approximately
                    8823 links and the exact same amount for non-links (therefore we could not use 1622124 non-links ,
                    since it would make the dataset heavily imbalanced). The equilibrium must be 50%/50% for the best
                    results, therefore, the number of links limits the number of instances we may use for both links and
                    non links on the training dataset. This strategy is focused on maintained fairness for the predictions
                    to be performed by our classifiers;
                </li>
                <li>
                    an ideal sampling rate would be 50%, in addition, the target links and non-links meant for prediction
                    should be left out of the training. Therefore, if we are really interested in predicting if a non-link
                    could actually be a yet unknown link in the graph, we must leave it out of the training;
                </li>
                <li>
                    an example application of link prediction is: suppose you are working with chemistry and you'd like
                    to know which chemical compounds react with each other. For that, you build a graph where each vertex
                    represents a chemical compound and each link represents that those two compounds react together. Note
                    that you'd only create links between compounds you know for sure to react together, while the compounds
                    which you know for sure that do not react and the ones which you don't know if they react, you leave
                    them without edges. As you have realized by now, comparing all 1000 chemical compounds with each
                    other results in 499500 unique chemical tests to be performed (excluding tests with a chemical against
                    itself), which can extremely expensive and time demanding. Link prediction would accelerate and make
                    cheaper this process, it would give you the links which are likely to exist, reducing the practical
                    (and potentially very expensive) testings to a much smaller set of chemicals combinations;
                </li>
            </ul>

            It's time to dive into the code, here it's proposed using the XGBoost and a Transformer Neural Network to see
            the results. Let's start by importing the required libraries.
<pre><code class="lang-python">import numpy as np
import networkx as nx
from nbne import train_model
from os.path import join, abspath, dirname
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score
from time import time
import xgboost
import tensorflow as tf
import os
import warnings
import xgboost_gpu
</code></pre>
            The file "xgboost_gpu" was created in this <a href="dask_xgboost_gpu.html" rel="noopener noreferrer" target="_blank">article</a>
            and is meant to facilitate the usage of XGBoost in the GPU.

            Now let's create a function to facilitate obtaining each vertex emebedding from the NBNE, it
            returns a 2D <code class="inline_code">numpy.ndarray</code> where the i-th row represents the i-th node in the
            <code class="inline_code">networkx.Graph</code> "graph" and the respective associated columns represents the
            embeddings of vertex i.
<pre><code class="lang-python">def get_nbne_embeddings(
            graph,
            num_permutations=10,
            embedding_dimension=128,
            window_size=5,
            min_count=0,
            min_degree=0,
            workers=8,
            dtype=np.float64
            ):
    '''
    Returns a 2D numpy.ndarray where the i-th row represent's the i-th node in the networkx.Graph "graph" and the
    respective associated columns represents the embeddings of vertex i. Note that this function considers the names of
    all nodes in the graph to be integers in the range [0, number of nodes in the graph], that allows for the
    abstraction of mapping each vertex to a unique position in the numpy.ndarray, creating an efficient data structure
    for later usage;
    :param graph: a networkx.Graph representing the graph to extract the embeddings from;
    :param num_permutations: a NBNE's parameter defining the number of permutations performed using the neighborhood of
        a vertex;
    :param embedding_dimension: the output number of dimensions for the embeddings of each vertex;
    :param window_size: the window size used to predict a node given it's neighbours;
    :param min_count: ignores all vertices with total frequency lower than this. Frequency here is considering the
        occurrence of a vertex as a neighbour of other vertices;
    :param min_degree: ignores all vertices with degree lower than this;
    :param workers: number of threads Gensim's training uses (it's called by the NBNE internally);
    :param dtype: the data type of the output embeddings numpy.ndarray;
    :return:
        * emb: a 2D numpy.ndarray where the i-th row represent's the i-th node in the networkx.Graph "graph" and the
        respective associated columns represents the embeddings of vertex i. This structure is shaped as follows:
        (number of vertices in the graph, embedding_dimension);
    '''
    model = train_model(
                        graph,
                        num_permutations=num_permutations,
                        output_file=None,
                        embedding_dimension=embedding_dimension,
                        window_size=window_size,
                        min_count=min_count,
                        min_degree=min_degree,
                        workers=workers
                        )
    emb = np.empty(shape=(graph.number_of_nodes(), embedding_dimension), dtype=dtype)
    for word in model.wv.index2entity:
        emb[int(word), :] = model.wv.word_vec(word=word)
    return emb
</code></pre>

            Following, let's create another function to facilitate reading the input graph from a file, loading it
            into a <code class="inline_code">networkx.Graph</code> structure:

<code class="lang-python"><pre>def _read_graph(file_name, adj_matrix_dtype=np.float64):
	'''
	Retrieves from a graph from an "edgelist" file (check networkx documentation for more details on this specific file
	format) specified by it's name without the associated operating system path. The file will be searched in the
	"graphs" folder of this project, no error handling is performed.
	:param file_name: the raw file name, i. e., without it's path. The path is assumed to be the "graphs" folder
		inside this project on a cross-platform fashion;
	:param adj_matrix_dtype: the data type to of the output "A" matrix;
	:return:
	Returns two versions of the same undirected graph:
		* G: a networkx.Graph instance containing the file's structured data;
		* A: a numpy.ndarray containing the equivalent adjacency matrix. This structure is mean't for greater
			efficiency when generating the training data for link prediction;
	'''
	full_path = join(join(dirname(dirname(abspath(__file__))), "graphs"), file_name)
	G = nx.read_edgelist(full_path)
	A = nx.to_numpy_array(G, dtype=adj_matrix_dtype)
	return G, A
</code></pre>

            Now let's create a function which takes as input an adjacency matrix representation of the graph and another
            matrix mapping each vertex to it's corresponding embeddings and create a dataset contaning exactly 50%/50%
            equilibrium between edges and non-edges. Keep in mind that for the scope of this work, we do not need to
            give special treatment for any vertices in particular since we are not looking to solve problems related
            to any of them in particular. In a real world problem, there could be vertices and edges requiring special
            attention and, therefore, one might need to exclude from the traning data whatever characteristics that are
            being targeted for prediction.


<code class="lang-python"><pre>def assemble_balanced_link_prediction_data(A, emb, sampling_rate=0.5, dtype=np.float32):
	'''
	Generates class balanced training/testing data for graph "A" using it's embeddings "emb". This function
	is meant to be used with
	undirected unweighted/weighted graphs, since it only considers the upper diagonal of "A" due to efficiency purposes.
	Self loops are also ignored;
	:param A: a square 2D numpy.ndarray representing the graph's adjacency matrix;
	:param emb: a 2D numpy.ndarray where each row represents a vertex from A and the columns represents the embeddings
		matching the vertex in the same row;
	:param sampling_rate: a float in the interval (0.0, 1.0] determining the rate at which links and non-links will be
		sampled from the graph to be used in the training data produced;
	:param dtype: the data type of the produced training data;
	:return:
		* X_train: a 2D numpy.ndarray representing the training features;
		* y_train: a 1D numpy.ndarray representing the training labels/targets;
		'''
	n = A.shape[0]
	max_instances = int((((n ** 2) - n) / 2) * sampling_rate)
	zeros_idx, ones_idx = [], []
	for i in range(n):
		for j in range(i + 1, n - 1):
			if float(A[i, j]) == 0.0:
				zeros_idx.append((i, j, 0))
			else:
				ones_idx.append((i, j, 1))
	dominant_class = 1 if len(ones_idx) > len(zeros_idx) else 0
	np.random.shuffle(zeros_idx)
	np.random.shuffle(ones_idx)
	if dominant_class == 0:
		zeros_idx = zeros_idx[:len(ones_idx)]
	else:
		ones_idx = ones_idx[:len(zeros_idx)]
	if len(zeros_idx) + len(ones_idx) > max_instances:
		diff = np.ceil(np.abs(len(zeros_idx) + len(ones_idx) - max_instances) / 2)
		zeros_idx = zeros_idx[:-diff]
		ones_idx = ones_idx[:-diff]

	data = np.empty(shape=(len(zeros_idx) + len(ones_idx), 2 * emb.shape[1] + 1), dtype=dtype)
	for k, (i, j, label) in enumerate(zeros_idx + ones_idx):
		data[k, :emb.shape[1]] = emb[i, :]
		data[k, emb.shape[1]:-1] = emb[j, :]
		data[k, -1] = label
	np.random.shuffle(data)
	x = data[:, :-1]
	y = data[:, -1]

	return x, y
</code></pre>

            Time for another convenience function, just so a few relevant scores may be properly printed to the screen.
            Since this is an imbalanced class problem, it's very interesting to take a look at metrics beyond the accuracy,
            such as ROC AUC, F1, precision and recall.

<code class="lang-python"><pre>def print_scores_summary(ytrue, ypredicted, training_time, inference_time, model_name="Model"):
	def fmt_fixed_len(str, fixed_length=25):
		assert fixed_length >= len(str)
		n = fixed_length - len(str)
		return str + (" " * n)

	print("{} {} results {}".format("=" * 5, model_name, "=" * 5))
	print("{}{:.2f}%".format(fmt_fixed_len("Accuracy:"), accuracy_score(ytrue, ypredicted) * 100))
	print("{}{:.2f}%".format(fmt_fixed_len("ROC AUC:"), roc_auc_score(ytrue, ypredicted) * 100))
	print("{}{:.2f}%".format(fmt_fixed_len("F1:"), f1_score(ytrue, ypredicted) * 100))
	print("{}{:.2f}%".format(fmt_fixed_len("Precision:"), precision_score(ytrue, ypredicted) * 100))
	print("{}{:.2f}%".format(fmt_fixed_len("Recall:"), recall_score(ytrue, ypredicted) * 100))
	print("{} training time: {:.2f} seconds.".format(model_name, training_time))
	print("{} inference time: {:.2f} seconds.".format(model_name, inference_time))
</code></pre>
            Another convenience function to create a deep learning model: a transformer neural network (NN). It's important
            to notice that this transformer is composed only of the encoding part of a complete transformer model,
            where the fully connected layers at the top (close to outputs of this NN) work as a decoder of some sort,
            producing answers. The input features will be assumed to be embeddings in a positional encoding configuration.

<code class="terminal_output"><pre>def make_nn(input_size):
	model_inputs = tf.keras.Input(shape=input_size, dtype=tf.float32)
	#layer = tf.keras.layers.Reshape((2, -1))(model_inputs)
	half = int(input_size / 2)
	layer0 = tf.keras.layers.Reshape((1, -1))(model_inputs[:, :half])
	layer1 = tf.keras.layers.Reshape((1, -1))(model_inputs[:, half:])

	a0 = tf.keras.layers.MultiHeadAttention(num_heads=16, key_dim=16, value_dim=16, dropout=0.0)(layer0, layer0)
	a00 = tf.keras.layers.LayerNormalization()(a0 + layer0)
	a0 = tf.keras.layers.Concatenate(axis=1)([a0, a00])
	a1 = tf.keras.layers.MultiHeadAttention(num_heads=16, key_dim=16, value_dim=16, dropout=0.0)(layer1, layer1)
	a11 = tf.keras.layers.LayerNormalization()(a1 + layer1)
	a1 = tf.keras.layers.Concatenate(axis=1)([a1, a11])
	a2 = tf.keras.layers.MultiHeadAttention(num_heads=16, key_dim=16, value_dim=16, dropout=0.0)(layer0, layer1)
	a22 = tf.keras.layers.LayerNormalization()(a2 + layer0 + layer1)
	a2 = tf.keras.layers.Concatenate(axis=1)([a2, a22])
	a3 = tf.keras.layers.MultiHeadAttention(num_heads=16, key_dim=16, value_dim=16, dropout=0.0)(layer1, layer0)
	a33 = tf.keras.layers.LayerNormalization()(a3 + layer1 + layer0)
	a3 = tf.keras.layers.Concatenate(axis=1)([a3, a33])
	layer = tf.keras.layers.Concatenate(axis=1)([a0, a1, a2, a3])

	partially_encoded = tf.keras.layers.Flatten()(layer)

	layer = tf.keras.layers.Dense(128, activation="relu")(partially_encoded)
	layer = tf.keras.layers.Dense(partially_encoded.shape[-1], activation="relu")(layer)
	layer = tf.keras.layers.LayerNormalization()(layer + partially_encoded)
	layer = tf.keras.layers.Concatenate(axis=-1)([layer, partially_encoded])
	model_outputs = tf.keras.layers.Dense(128, activation="sigmoid")(layer)

	model_outputs = tf.keras.layers.Dense(1, activation="sigmoid")(layer)

	model = tf.keras.Model(inputs=model_inputs, outputs=model_outputs)
	model.compile(optimizer="adam",
	              loss="binary_crossentropy",
	              metrics=["binary_accuracy", "AUC", tf.keras.metrics.FalsePositives(), tf.keras.metrics.TruePositives()]
	              )
	return model
</code></pre>

            Finally, it's time to run the experiments. For better statistical relevance results, we should employ the
            cross validation technique, however, only holdout will be employed, since this script will take quite some
            time to execute and the focus of this tutorial is not on obtainng scores with high statistical quality.

<code class="lang-python"><pre>if __name__ == "__main__":
	start_time = time()
	EMB_DIM_COUNT = 256
	BATCH_SIZE = 128
	with warnings.catch_warnings():
		warnings.simplefilter("ignore")
		"""
		graph retrieved from the page "https://snap.stanford.edu/data/egonets-Facebook.html", by downloading the file
		"facebook_combined.txt.gz". The full download link is "https://snap.stanford.edu/data/facebook_combined.txt.gz".
		"""
		read_graph_tstart = time()
		graph_file_name = "facebook_combined.txt"
		G, A = _read_graph(graph_file_name)
		print("Graph read in: {:.2f} seconds".format(time() - read_graph_tstart))
		emb_tstart = time()
		emb = get_nbne_embeddings(
									G,
									num_permutations=10,
									embedding_dimension=EMB_DIM_COUNT,
									window_size=5,
									min_count=0,
									min_degree=3,
									workers=8
									)
		# normalizing the embeddings should, generally, be a good a idea
		emb = normalize(emb)
		print("Vertices embeddings trained in: {:.2f} seconds".format(time() - emb_tstart))
		# here, one might use sampling_rate=1.0 to combine the links and non-links between all vertices, then the data is
		# split among training and testing by the sklearn.train_test_split
		assemble_tstart = time()
		x, y = assemble_balanced_link_prediction_data(A, emb, sampling_rate=0.10, dtype=np.float64)
		xtrain, xtest, ytrain, ytrue = train_test_split(x, y, test_size=1/4, shuffle=True)
		print("Training and testing data assembled in: {:.2f} seconds".format(time() - assemble_tstart))

		xgb_train_tstart = time()
		xgb_dict = xgboost_gpu.train_xgboost_gpu(
										xtrain,
										ytrain,
										data_chunksize=None,
										n_gpus=2,
										n_threads_per_gpu=10,
										params={"objective": "binary:logistic", "eval_metric": "logloss"}
										)
		xgb_model = xgb_dict['booster']
		xgb_train_telapsed = time() - xgb_train_tstart
		xgb_inference_tstart = time()

		ypredicted = xgboost_gpu.predict_xgboost_gpu(
										xgb_model,
										xtest,
										data_chunksize=None,
										n_gpus=1,
										n_threads_per_gpu=20,
										)
		ypredicted = np.round(ypredicted)
		xgb_inference_telapsed = time() - xgb_inference_tstart
		print_scores_summary(ytrue, ypredicted, xgb_train_telapsed, xgb_inference_telapsed, model_name="XGBoost (GPU)")

		xgb_inference_tstart = time()
		ypredicted = xgb_model.predict(xgboost.DMatrix(xtest, feature_names=[str(k) for k in range(xtest.shape[1])]))
		ypredicted = np.round(ypredicted)
		print("XGBoost (CPU) inference time: {:.2f} seconds.".format(time() - xgb_inference_tstart))

		# =============================================================================
		model = make_nn(EMB_DIM_COUNT * 2)
		print(model.summary())

		def scheduler(epoch, lr):
			return lr * tf.math.exp(-0.06)
		lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)

		transformer_train_tstart = time()
		model.fit(xtrain, ytrain, epochs=50, batch_size=BATCH_SIZE, callbacks=[lr_scheduler])
		cnn_train_telapsed = time() - transformer_train_tstart

		cnn_inference_tstart = time()
		ypredicted = model.predict(xtest)
		ypredicted = np.round(ypredicted)
		cnn_iference_telapsed = time() - cnn_inference_tstart
		print_scores_summary(ytrue, ypredicted, cnn_train_telapsed, cnn_iference_telapsed, model_name="Transformer")

		print("Total script execution time: {:.2f} seconds.".format(time() - start_time))
</code></pre>


            After removing lots of warnings, the output of this script should be similar to the following:

<code class="terminal_output"><pre>Graph read in: 0.33 seconds
Vertices embeddings trained in: 27.69 seconds
Training and testing data assembled in: 7.97 seconds
[15:07:19] task [xgboost.dask]:tcp://127.0.0.1:32897 got new rank 0
[15:07:19] task [xgboost.dask]:tcp://127.0.0.1:37085 got new rank 1
===== XGBoost (GPU) results =====
Accuracy:                87.24%
ROC AUC:                 87.26%
F1:                      87.88%
Precision:               83.46%
Recall:                  92.79%
XGBoost (GPU) training time: 83.32 seconds.
XGBoost (GPU) inference time: 12.30 seconds.
XGBoost (CPU) inference time: 0.29 seconds.

*** lots of hidden warnings in this section ***

Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 512)]        0
__________________________________________________________________________________________________
tf.__operators__.getitem (Slici (None, 256)          0           input_1[0][0]
__________________________________________________________________________________________________
tf.__operators__.getitem_1 (Sli (None, 256)          0           input_1[0][0]
__________________________________________________________________________________________________
reshape (Reshape)               (None, 1, 256)       0           tf.__operators__.getitem[0][0]
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 1, 256)       0           tf.__operators__.getitem_1[0][0]
__________________________________________________________________________________________________
multi_head_attention_2 (MultiHe (None, 1, 256)       263168      reshape[0][0]
                                                                 reshape_1[0][0]
__________________________________________________________________________________________________
multi_head_attention_3 (MultiHe (None, 1, 256)       263168      reshape_1[0][0]
                                                                 reshape[0][0]
__________________________________________________________________________________________________
multi_head_attention (MultiHead (None, 1, 256)       263168      reshape[0][0]
                                                                 reshape[0][0]
__________________________________________________________________________________________________
multi_head_attention_1 (MultiHe (None, 1, 256)       263168      reshape_1[0][0]
                                                                 reshape_1[0][0]
__________________________________________________________________________________________________
tf.__operators__.add_2 (TFOpLam (None, 1, 256)       0           multi_head_attention_2[0][0]
                                                                 reshape[0][0]
__________________________________________________________________________________________________
tf.__operators__.add_4 (TFOpLam (None, 1, 256)       0           multi_head_attention_3[0][0]
                                                                 reshape_1[0][0]
__________________________________________________________________________________________________
tf.__operators__.add (TFOpLambd (None, 1, 256)       0           multi_head_attention[0][0]
                                                                 reshape[0][0]
__________________________________________________________________________________________________
tf.__operators__.add_1 (TFOpLam (None, 1, 256)       0           multi_head_attention_1[0][0]
                                                                 reshape_1[0][0]
__________________________________________________________________________________________________
tf.__operators__.add_3 (TFOpLam (None, 1, 256)       0           tf.__operators__.add_2[0][0]
                                                                 reshape_1[0][0]
__________________________________________________________________________________________________
tf.__operators__.add_5 (TFOpLam (None, 1, 256)       0           tf.__operators__.add_4[0][0]
                                                                 reshape[0][0]
__________________________________________________________________________________________________
layer_normalization (LayerNorma (None, 1, 256)       512         tf.__operators__.add[0][0]
__________________________________________________________________________________________________
layer_normalization_1 (LayerNor (None, 1, 256)       512         tf.__operators__.add_1[0][0]
__________________________________________________________________________________________________
layer_normalization_2 (LayerNor (None, 1, 256)       512         tf.__operators__.add_3[0][0]
__________________________________________________________________________________________________
layer_normalization_3 (LayerNor (None, 1, 256)       512         tf.__operators__.add_5[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 2, 256)       0           multi_head_attention[0][0]
                                                                 layer_normalization[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 2, 256)       0           multi_head_attention_1[0][0]
                                                                 layer_normalization_1[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 2, 256)       0           multi_head_attention_2[0][0]
                                                                 layer_normalization_2[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 2, 256)       0           multi_head_attention_3[0][0]
                                                                 layer_normalization_3[0][0]
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 8, 256)       0           concatenate[0][0]
                                                                 concatenate_1[0][0]
                                                                 concatenate_2[0][0]
                                                                 concatenate_3[0][0]
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2048)         0           concatenate_4[0][0]
__________________________________________________________________________________________________
dense (Dense)                   (None, 128)          262272      flatten[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2048)         264192      dense[0][0]
__________________________________________________________________________________________________
tf.__operators__.add_6 (TFOpLam (None, 2048)         0           dense_1[0][0]
                                                                 flatten[0][0]
__________________________________________________________________________________________________
layer_normalization_4 (LayerNor (None, 2048)         4096        tf.__operators__.add_6[0][0]
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 4096)         0           layer_normalization_4[0][0]
                                                                 flatten[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            4097        concatenate_5[0][0]
==================================================================================================
Total params: 1,589,377
Trainable params: 1,589,377
Non-trainable params: 0
__________________________________________________________________________________________________
None

*** lots of hidden warnings in this section ***


1034/1034 [==============================] - 16s 10ms/step - loss: 0.5300 - binary_accuracy: 0.7378 - auc: 0.8042 - false_positives: 10155.0261 - true_positives: 26739.1092
Epoch 2/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4662 - binary_accuracy: 0.7761 - auc: 0.8407 - false_positives: 10291.5981 - true_positives: 28671.6580
Epoch 3/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4491 - binary_accuracy: 0.7840 - auc: 0.8507 - false_positives: 10423.8000 - true_positives: 29261.0126
Epoch 4/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4374 - binary_accuracy: 0.7910 - auc: 0.8589 - false_positives: 10240.7614 - true_positives: 29658.6850
Epoch 5/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4314 - binary_accuracy: 0.7955 - auc: 0.8631 - false_positives: 10011.8541 - true_positives: 29622.2763
Epoch 6/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4206 - binary_accuracy: 0.8003 - auc: 0.8694 - false_positives: 9861.7014 - true_positives: 29771.5884
Epoch 7/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4154 - binary_accuracy: 0.8031 - auc: 0.8730 - false_positives: 9825.8048 - true_positives: 30090.4271
Epoch 8/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4043 - binary_accuracy: 0.8095 - auc: 0.8790 - false_positives: 9606.1662 - true_positives: 30175.4734
Epoch 9/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.4000 - binary_accuracy: 0.8108 - auc: 0.8819 - false_positives: 9511.9643 - true_positives: 30105.1217
Epoch 10/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3926 - binary_accuracy: 0.8138 - auc: 0.8863 - false_positives: 9363.8609 - true_positives: 30220.8531
Epoch 11/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3874 - binary_accuracy: 0.8185 - auc: 0.8896 - false_positives: 9143.9159 - true_positives: 30308.2155
Epoch 12/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3831 - binary_accuracy: 0.8198 - auc: 0.8923 - false_positives: 9055.1778 - true_positives: 30305.8135
Epoch 13/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3753 - binary_accuracy: 0.8247 - auc: 0.8971 - false_positives: 8810.5246 - true_positives: 30335.1652
Epoch 14/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3685 - binary_accuracy: 0.8289 - auc: 0.9004 - false_positives: 8632.9681 - true_positives: 30458.7710
Epoch 15/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3629 - binary_accuracy: 0.8306 - auc: 0.9040 - false_positives: 8534.1720 - true_positives: 30439.8860
Epoch 16/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3577 - binary_accuracy: 0.8324 - auc: 0.9066 - false_positives: 8401.9498 - true_positives: 30514.8966
Epoch 17/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3533 - binary_accuracy: 0.8361 - auc: 0.9092 - false_positives: 8210.9700 - true_positives: 30512.3362
Epoch 18/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3442 - binary_accuracy: 0.8406 - auc: 0.9150 - false_positives: 7980.7865 - true_positives: 30400.4850
Epoch 19/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3387 - binary_accuracy: 0.8436 - auc: 0.9169 - false_positives: 7829.1681 - true_positives: 30645.7836
Epoch 20/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3355 - binary_accuracy: 0.8435 - auc: 0.9188 - false_positives: 7699.4628 - true_positives: 30484.4705
Epoch 21/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3292 - binary_accuracy: 0.8474 - auc: 0.9223 - false_positives: 7601.1778 - true_positives: 30668.9266
Epoch 22/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3228 - binary_accuracy: 0.8510 - auc: 0.9252 - false_positives: 7374.0570 - true_positives: 30619.6541
Epoch 23/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3166 - binary_accuracy: 0.8532 - auc: 0.9283 - false_positives: 7167.6918 - true_positives: 30633.4522
Epoch 24/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3111 - binary_accuracy: 0.8576 - auc: 0.9311 - false_positives: 6969.4425 - true_positives: 30728.1275
Epoch 25/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3068 - binary_accuracy: 0.8600 - auc: 0.9330 - false_positives: 6842.7498 - true_positives: 30655.6280
Epoch 26/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.3007 - binary_accuracy: 0.8626 - auc: 0.9360 - false_positives: 6666.9024 - true_positives: 30776.8309
Epoch 27/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2958 - binary_accuracy: 0.8656 - auc: 0.9381 - false_positives: 6535.6609 - true_positives: 30812.5353
Epoch 28/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2920 - binary_accuracy: 0.8663 - auc: 0.9397 - false_positives: 6408.8386 - true_positives: 30698.3034
Epoch 29/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2820 - binary_accuracy: 0.8732 - auc: 0.9444 - false_positives: 6167.8870 - true_positives: 30931.4812
Epoch 30/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2783 - binary_accuracy: 0.8750 - auc: 0.9459 - false_positives: 6059.2222 - true_positives: 30877.8676
Epoch 31/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2710 - binary_accuracy: 0.8771 - auc: 0.9489 - false_positives: 5938.6618 - true_positives: 30905.1546
Epoch 32/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2642 - binary_accuracy: 0.8813 - auc: 0.9516 - false_positives: 5741.9159 - true_positives: 30870.9362
Epoch 33/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2613 - binary_accuracy: 0.8825 - auc: 0.9526 - false_positives: 5658.9884 - true_positives: 30971.6213
Epoch 34/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2560 - binary_accuracy: 0.8863 - auc: 0.9548 - false_positives: 5460.2696 - true_positives: 31057.9691
Epoch 35/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2546 - binary_accuracy: 0.8860 - auc: 0.9552 - false_positives: 5440.5507 - true_positives: 31082.5565
Epoch 36/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2500 - binary_accuracy: 0.8895 - auc: 0.9568 - false_positives: 5302.9082 - true_positives: 31111.1372
Epoch 37/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2425 - binary_accuracy: 0.8936 - auc: 0.9595 - false_positives: 5127.5034 - true_positives: 31119.8696
Epoch 38/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2393 - binary_accuracy: 0.8946 - auc: 0.9609 - false_positives: 5014.7304 - true_positives: 31248.2077
Epoch 39/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2372 - binary_accuracy: 0.8952 - auc: 0.9614 - false_positives: 5014.2329 - true_positives: 31266.5169
Epoch 40/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2324 - binary_accuracy: 0.8988 - auc: 0.9633 - false_positives: 4820.1990 - true_positives: 31234.4831
Epoch 41/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2275 - binary_accuracy: 0.9006 - auc: 0.9652 - false_positives: 4756.8454 - true_positives: 31371.3585
Epoch 42/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2262 - binary_accuracy: 0.9019 - auc: 0.9653 - false_positives: 4653.3594 - true_positives: 31259.6164
Epoch 43/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2228 - binary_accuracy: 0.9038 - auc: 0.9668 - false_positives: 4614.2638 - true_positives: 31336.7855
Epoch 44/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2163 - binary_accuracy: 0.9061 - auc: 0.9686 - false_positives: 4508.0589 - true_positives: 31353.1295
Epoch 45/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2174 - binary_accuracy: 0.9052 - auc: 0.9683 - false_positives: 4497.6338 - true_positives: 31422.6155
Epoch 46/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2133 - binary_accuracy: 0.9091 - auc: 0.9696 - false_positives: 4326.9874 - true_positives: 31362.6300
Epoch 47/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2117 - binary_accuracy: 0.9094 - auc: 0.9705 - false_positives: 4297.6763 - true_positives: 31451.0039
Epoch 48/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2082 - binary_accuracy: 0.9111 - auc: 0.9713 - false_positives: 4250.7536 - true_positives: 31519.3575
Epoch 49/50
1034/1034 [==============================] - 11s 10ms/step - loss: 0.2047 - binary_accuracy: 0.9133 - auc: 0.9724 - false_positives: 4169.1855 - true_positives: 31545.2029
Epoch 50/50
1034/1034 [==============================] - 10s 10ms/step - loss: 0.2041 - binary_accuracy: 0.9134 - auc: 0.9724 - false_positives: 4155.9420 - true_positives: 31504.6271
===== Transformer results =====
Accuracy:                83.96%
ROC AUC:                 83.98%
F1:                      84.55%
Precision:               81.33%
Recall:                  88.03%
Transformer training time: 536.54 seconds.
Transformer inference time: 4.41 seconds.


Total script execution time: 674.73 seconds.
</code></pre>
			Note that the above code was executed in a very powerful machine running Ubuntu 16.04 with 128 GB of
			RAM, 10-cores 20-threads CPU, 2 GPUs RTX 3090 with 24 GB of dedicated RAM each, multiple SSDs, among more.

			<h3>Source-code</h3>
			Available on <a href="https://github.com/gmmoliveira/link-prediction" target="_blank">my github page</a>,
			written in Python.
			A documentation of the code is provided in the "README.md" file, as well as docstrings in the source-code
			itself.

			<h3>Requirements</h3>
			<ul>
				<li>python==3.8.3</li>
				<li>gensim==3.8.3</li>
				<li>dask==2.30.0</li>
				<li>networkx==2.5</li>
				<li>pandas==1.1.3</li>
				<li>dask_cuda==0.18.0</li>
				<li>xgboost==1.3.3</li>
				<li>nbne==0.81</li>
				<li>tensorflow==2.4.1</li>
				<li>numpy==1.19.2</li>
				<li>scikit_learn==0.24.1</li>
			</ul>

			<h3>Conclusion</h3>

			<p>
				Working with imbalanced problems in machine learning is difficult, in general. It's generally a good idea to
				ensure that classes are balanced so the models may evenly learn internal characteristics which fairly
				favors all the output classes, however, that doesn't mean the models will output all classes with
				the same probability distribution as the training set. The difference, as already mentioned and it must be hightlighted,
				lies in the learned elements from the dataset, even though the predictions will be performed on a
				imbalanced set of samples.
			</p>
			<p>
				Further, there are more complex link prediction problems which require more advanced techniques. For an example,
				we could be dealing with graphs which carry more information than just connections between vertices, they could
				contain vectors associated to each vertex and/or edge, the graph could also be a multigraph where there are
				multiple types of vertices and edges interacting together.
			</p>

            <br><br><br>
            <hr>
			Cheers!
			<br>
			gmmoliveira1@gmail.com
		</div>

		<div id="page_dates">
			Page created on <b>August 11, 2020</b>
			<br>
			Page last updated on <b>April 08, 2021</b>
		</div>

		<div id="footer">
			Web site created by Guilherme Oliveira, it may be used by anyone, free of charge. See <a class="footer_links" href="LICENSE.html" rel="noopener noreferrer" target="_blank">license</a>.
            <hr>
			Copyright <sup>&#9400;</sup> 2020 Guilherme Oliveira
			<br>
			SPDX-License-Identifier: <a class="footer_links" href="https://www.apache.org/licenses/license-2.0" rel="noopener noreferrer" target="_blank">Apache-2.0</a>
		</div>
	</body>
</html>
